\chapter{Solution Design and Implementation}\label{chp:design-and-impl}

\section{Problem Definition}
The purpose of this study is to investigate whether it is possible to predict whether a bug is likely to occur in a given file committed to any source code repository from a number of quality metrics associated with each file. 

\section{Solution Design}\label{sec:design}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Data Gathered}\label{sec:data-available}
The data collected contained \totalRows{} initially collected in 67 columns. Each record corresponds to a source code file at a given point in time and subsection \ref{sec:data-available} depicts attributes definition through Tables \ref{tbl:available-data-non-repeating-types} to \ref{tbl:available-data-above-0-max-100}, specifying uncommon datatypes found, as well as lists where each attribute in the list has the same properties with regards to data category, data type, and minimum and maximum values present. 

\begin{table}[h!]
\caption{Data Available - non repeating data types}
\label{tbl:available-data-non-repeating-types}
\begin{tabular}{@{}ll@{}}
\toprule
Metric name & Description \\ \midrule
issue key & text \\ 
file path & text \\
source repo & text \\
author & discrete, enumeration of possible authors \\
prev author & discrete, enumeration of possible authors\\
timestamp & continuous, epoch time \\
prev timestamp & continuous, epoch time \\
is bug & boolean, True or False \\
files & ordinal, integer, $\geq{}1$ \\
sqale debt & ratio, integer, $\geq{}0$ \\
statements & ordinal, integer, $\geq{}1$ \\
violations & ratio, integer, $\geq{}0$ \\ \bottomrule
\end{tabular}
\end{table}

\begin{table}[h!]
\caption{Ordinal data, integer, min $\geq{}0 \leq{}100$}
\label{tbl:available-data-above-0-max-100}
\begin{tabular}{@{}l@{}}
\toprule
Metric Name \\ \midrule
branch coverage \\
overall branch coverage \\
overall coverage \\
overall line coverage \\
overall uncovered conditions \\
overall uncovered lines \\ \bottomrule
\end{tabular}
\end{table}


\begin{table}[h!]
\caption{Numeric data, integer, min $\geq{}0$, no definitive maximum}
\label{tbl:available-data-above-0-no-max}
\begin{tabular}{@{}ll@{}}
\toprule
Metric Name & Metric Name \\ \midrule
blocker violations & line coverage \\
bugs & lines \\
classes & lines to cover \\
code smells & major violations \\
comment lines & minor violations \\
comment lines density & ncloc \\
complexity & open issues \\
confirmed issues & public documented API density \\
coverage & public undocumented API \\
critical violations & reliability rating \\
duplicated blocks & reliability remediation effort \\
duplicated files & reopened issues \\
duplicated lines & security rating \\
duplicated lines density & security remediation effort \\
effort to reach maintainability rating A & skipped tests \\
false positive issues & SQALE index \\
file complexity & SQALE rating \\
function complexity & test errors \\
functions & test failures \\
generated lines & test success density \\
generated ncloc & uncovered conditions \\
info violations & uncovered lines \\
it coverage & vulnerabilities \\
it line coverage & wont fix issues \\
it uncovered lines &  \\ \bottomrule
\end{tabular}
\end{table}


\subsection{Data Gathering Components}
The dataset for the analysis had to be generated from live business data as such data has not been made available in the public domain. Section \ref{sec:data-available} can be used as a reference as to what metrics, along with their type and a brief description, have been gathered. In order to construct the dataset subsequently used in the analysis, 3 real live business tools were used:

\begin{enumerate}{\label{lst:tools_used}}
    \item Sonar server by SonarSource - provided code quality metrics
    \item JIRA by Atlassian  - provided information about file commits as well as the classification of commits into "a bug" or "not a bug" categories
    \item Bitbucket by Atlassian  - provided information about commit timestamp, the author as well as details about previous commit author and previous commit timestamp allowing for calculating file staleness 
\end{enumerate}
    
Additionally, a tool to gather metrics, referred to as Data Gatherer, has been developed in order to produce the end dataset and will be made available upon submission. The purpose of the Data Gatherer tool is to coordinate the execution of other tools from the above list as well as collate the acquired data into the end dataset then used in the further analysis as per Figure \ref{fig:data_gathering}.

\begin{figure}[h!]
\centering
    \includegraphics{\figpath/data_arch_small.png}
    \caption{Dataset Gathering operations}
    \label{fig:data_gathering}
\end{figure}

The first tool to be used in the data gathering process is Atlassian's JIRA server. JIRA is a ticketing system used by software developers to keep track of issues, feature requests and most importantly in the current context, bugs. Each ticket has been assigned a category upon its creation depicting its purpose. In the context of the current use case the most relevant types encountered are:

\begin{itemize}
    \item Story - represents a feature task to be done. Upon completion, it is a fully functional, testable vertical slice of functionality delivering value to end-users
    \item Task - represents an additional development task, such as, but not limited to, setting up test environments, automation of effort, including testing effort, infrastructure maintenance, etc. Does not provide direct value to end users
    \item Bug - represents issues found in released software, especially when expected and the actual behaviour of a given piece of functionality doesn't match.
\end{itemize}
Given that Task type tickets are not directly linked to the development life cycle they have been excluded from further analysis and the focus has been placed on Story and Bug type tickets instead.

The second use of the JIRA server was its proprietary integration with the source code repository - Bitbucket, as both tools are offered by the same company, Atlassian. For each of the JIRA tickets there existed a link between a given ticket and source code file changes, or commits, made and persisted in Bitbucket system. 
Each commit could consist of many files and each source code file could have been committed to multiple times under the umbrella of a single ticket. However, not all commit records under a ticket have been taken into the account - every so-called merge commit have been excluded.

Figure \ref{fig:git-branching-merging-and-rebasing} has been used To illustrate what a merge commit is and why it must be excluded. In the figure the blue nodes signify the Master or the golden copy of the code, whereas green and purple ones identify branches, where feature, bug fix or any other development work is carried out as part of a development lifecycle. At the end of its life, a branch will be integrated into the Master branch, which will generate a merge commit - illustrated by the Feature branch. Additionally, it is a normal occurrence to have more than 1 branch created in a given codebase at any time. If another branch has been created from the Master before the Feature code was merged, as illustrated in Figure \ref{fig:git-branching-merging-and-rebasing} by the purple Bug nodes, and it is actively being developed on past the merge point such branch will have to be rebased. The rebase process will pull in the merge commit created by Feature into the Bug branch, however, all source code files listed under merge commit will have already been analyzed under JIRA ticket representing the Feature. Therefore, it should be excluded from the further analysis as including it would effectively create duplicate records to analyze. Additionally, failure to exclude the merge commits also represented significant risk with regards to story vs. bug categorization of source code files committed as files modified under the Bug ticket might as well have been modified in Feature merge commit. In such scenario file committed under the Feature ticked would be categorized as non-bug, as per ticket category, as well as a bug as per category assigned by the second ticket.

\begin{figure}[!h]
    \centering
    \includegraphics[scale=0.6]{\figpath/git_branching.png}
    \caption{GIT branching and rebasing strategy}
    \label{fig:git-branching-merging-and-rebasing}
\end{figure}

JIRA ticketing system provided the following columns:
\begin{itemize}
    \item issue key
    \item source repository
    \item file path
    \item category of the record, bug or not a bug
\end{itemize}

while the Bitbucket server provided additional information about:
\begin{itemize}\label{lst:design:info-from-bitbucket}
    \item author
    \item previous author
    \item commit timestamp
    \item timestamp of the previous commit
\end{itemize}

The final tool utilized was SonarQube by SonarSource, an open-source platform. Its capabilities include continuous inspection of code quality to perform automatic reviews with static analysis of code to detect code smells, reports on duplicated code, coding standards, unit tests, code coverage, code complexity, comments,  security vulnerabilities, and much more. In the context of this use case it was utilized to associate code metrics listed in Tables \ref{tbl:available-data-above-0-max-100} and \ref{tbl:available-data-above-0-no-max} as well as \files{}, \sqaleDebt{}, \statements{} and \violations{} provided in Table \ref{tbl:available-data-non-repeating-types}.

The \issueKey{}, \sourceRepository{} and \filePath{} attributes were only used to verify that a given data point can be traced back to a relevant JIRA ticket in order to ascertain its assigned category. 
% The \authorAttrib{} and \prevAuthorAttrib{} were used to populate seniority and project tenure features. 
Finally, \timestamp{} and \prevTimestamp{} were used to calculate staleness of the file between changes.

\subsection{Machine Learning Models Utilized}

\subsubsection{Logistic Regression}
Logistic regression is a type of predictive analysis. It has been selected as it is considered an appropriate classification method when the target variable is binary\cite{Hastie2009OnLogisticRegForBinaryTargetClass}\cite{ridgeEstInLogReg}. 
As part of the analysis, two regularization methods were utilized to both reduce the number of features and to programmatically detect features contributing the most toward the correct classification of the target variable. 

\subsubsection{Linear Model Regularization} 
Regularization is defined as introducing an additional term to the loss function, the prediction model, in order to prevent overfitting of same. During the analysis Lasso and Ridge techniques, referred to as L1 and L2 regularization respectively, were used due to the number of features under analysis as both focus on coefficient shrinking, however, Table \ref{tab:regularization_l1_vs_l2} outlines key differences between the two methods, starting with the metrics measured by each.

\begin{table}[h!]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
L1 & L2 \\ \midrule
Sum of weight & Sum of square of weights \\
Sparse outputs & Non-sparse output \\
Built-in feature selection & No feature selection \\
High sparsity for highly correlated features & \begin{tabular}[c]{@{}l@{}}Even coefficient distribution \\ for highly correlated features\end{tabular} \\
Can  interpret models with large feature sets & Main use case is preventing overfitting \\ \bottomrule
\end{tabular}
\caption{Summary of L1 vs L2 Regularization Techniques}
\label{tab:regularization_l1_vs_l2}
\end{table}

Both methods perform well in spite of the presence of correlated features: L1 by picking the most significant feature and zeroing the coefficient of the related features effectively excluding them from the prediction model, L2 by ensuring even distribution of the coefficients of the correlated features. 

Lasso stands for Least Absolute Shrinkage and it is defined as $RSS$ $+$ \textalpha{} $*$ (sum of absolute value of weights) or:
\begin{equation}\label{eq_lasso}
\sum^n_{i=1}(\hat{y_i}- y_i)^2 + \alpha \sum^n_{j=0}|w_j|
\end{equation}
where \textalpha{} refers to the factor of the penalty applied to a feature and $w_i$ refers to the weight of the feature\cite{Hastie2009SpringerOfLasso}.

Ridge regression works by adding a penalty factor to square of the magnitude of coefficients\cite{Hastie2009SpringerOfRidge}. It can be represented as:
\begin{equation}\label{eq_ridge}
\sum^n_{i=1}(\hat{y_i}- y_i)^2 + \alpha \sum^n_{j=0}w_j^2
\end{equation}
From the same definition it can be deducted that impact of \textalpha{} will be the same is both methods, namely:
\begin{enumerate}\label{list:impact-of-alpha}
\itemsep0em
\item\label{it:item1} $\alpha = 0$, is the same as linear regression, 
\item\label{it:item2} $\alpha = \infty$, the coefficients will be zero due to the infinite weighting on square coefficients anything less than 0 will make the objective infinite
\item\label{it:item3} $0 < \alpha < \infty$, the coefficients will be found between 0 and the ones obtained from a simple linear regression
\end{enumerate}

\subsubsection{Decision Tree Classifier}
Decision Tree for classification is a non-parametric method of supervised machine learning. Along with Decision Tree Regression, it was first proposed in 1984\cite{breimanCart1984} and it was intended as a top-down approach applied to a given set of data points. To find solutions a decision tree will make a number of sequential, hierarchical, or top-down, decisions about the outcome variable based on the predictor data. 

A very simplistic decision tree for classification has been represented in Figure \ref{fig:decision-tree-sample}. The tree is built top-down, starting from a root node corresponding to the best predictor for the dataset, which branches would extend downwards from. Going down the tree, depending on the position, the nodes would carry a different meaning. Terminal nodes, coloured in green, are points where no further decisions can be made. They are referred to as called leaf nodes. The white nodes represent the decision boundary defining the points at which a classification rule, or classifier, is executed. This method of execution serves in creating a consistent and standardized way of predicting what class should be applied to a given data point as the same rules are applied to any and all available data points.

\begin{figure}[!h]
    \centering
    \includegraphics[scale=0.6]{Figures/decision_tree_sample.png}
    \caption{Sample Decision Tree Classification of Iris Flowers}
    \label{fig:decision-tree-sample}
\end{figure}

As the generation the tree progresses the classifiers are applied, depending on the features contained in the dataset, and continuously applied to each subset to generate additional nodes iteratively \cite{Bertsimas2017Cart} in each step while at the same time keeping track of the end structure of the tree. To an extent the method could be compared to a flowchart with a significant distinction - no feedback loops are allowed. Therefore, the model is to some extent easy to explain as it is relatively uncomplicated to visualize the end structure of the tree, subject to the number of prediction-contributing features.

Given the mode of operation, the function implemented is step-wise, not linear, enabling the model to accommodate more complex relationships between the data. 
Additionally, it makes it less susceptible to the effects of feature collinearity, making it a highly valuable model candidate for datasets with high correlation factor values between features.

CART models, the decision tree classifier is part of, are models based purely on data from previous observations \cite{breimanCart1984} of a given target variable and no assumptions are made with regards to the distribution of errors or that of the data and it is considered a highly versatile model \cite{ensembleMethodsInMachineLearningDietterich} well able to explore the solution space without getting stuck in the local search optimum solution \cite{ensembleMethodsInMachineLearningDietterich}. However, at the same time, the model's reliance on previous observations indicates that the algorithm will terminate as soon as the optimal solution fitting the training data is found \cite{ensembleMethodsInMachineLearningDietterich} indicating that there is a possibility of an even more accurate solution to be devised in the future.

\subsection{Data Modelling}

Once a dataset has been generated, as per section \ref{sec:design}, it underwent the following operations:
\begin{enumerate}\label{lst:dataset-ops}
    \item missing data analysis and data cleaning \label{lst:dataset-ops.item:data-cleaning}
    \item balance of bug to non-bug records in the dataset \label{lst:dataset-ops.item:bug-to-non-bug-balance}
    \item outlier analysis \label{lst:dataset-ops.item:outliers}
    \item data correlation analysis \label{lst:dataset-ops.item:data-correlation}
    \item feature transformation \label{lst:dataset-ops.item:feature-transformation}
    \item data normalization \label{lst:dataset-ops.item:data-scaling}
    \item relevant feature selection \label{lst:dataset-ops.item:feature-selection}
    \item distribution of selected features \label{lst:dataset-ops.item:attribute-distribution}
    \item evaluation of regularized Logistic Regression model \label{lst:dataset-ops.item:ml-logistic-regression}
    \item evaluation of Decision Tree Classification model \label{lst:dataset-ops.item:ml-decision-tree}
\end{enumerate}

Item \ref{lst:dataset-ops.item:data-cleaning} focuses on cleaning and imputing values for missing metrics. Item \ref{lst:dataset-ops.item:bug-to-non-bug-balance} will focus on listing the ratios between bug and non bug records in the dataset.
Item \ref{lst:dataset-ops.item:outliers} identifies if there are any projects or individual files that are diverging significantly from the rest of the dataset. 
Item \ref{lst:dataset-ops.item:data-correlation} focuses on identifying any feature correlation patterns in order to proceed with feature selection - item  \ref{lst:dataset-ops.item:feature-selection} after which it will be necessary to check for the balance and distribution of the selected features or attributes - which will be the focus of item \ref{lst:dataset-ops.item:attribute-distribution}. Feature distribution is included as part of the analysis as Logistic Regression model is susceptible to non-normally distributed datasets.
Item \ref{lst:dataset-ops.item:data-scaling} concentrates on evaluating multiple scaling methods with regards to their effectiveness of bringing the dataset values to the same scale of magnitude. The scaling method taken into the account are:
\begin{enumerate}
    \item Transformation using natural logarithm - $log(e)$ 
    \item Min-Max scaling
    \item Decimal Scaling Normalization (also referred to as Max-Abs scaling)
    \item Z-Score Normalization (also referred to as Standard scaling)
    \item Power Transformation using Yeo-Johnson method
    \item Quantile Transformation using uniform distribution variant
    \item Quantile Transformation using normal distribution variant
\end{enumerate}

Finally, an evaluation of the effectiveness of selected machine learning models with regards to predicting bug vs non-bug classes in the dataset has been carried out and constitutes the focus of items \ref{lst:dataset-ops.item:ml-logistic-regression} and \ref{lst:dataset-ops.item:ml-decision-tree} respectively.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Data Normalization methods}\label{sec:data-modelling:scalers}
\paragraph{Min Max method}\label{sec:data-modelling:scalers:min-max}
Min-max data normalization is a method of scaling data by assigning it a value in a range of between 0 and 1. Alternatively, the value range of -1 to 1 can also be used \cite{dataNormalization2014}. The method normalizes the values of the attributes of a data set according to the minimum and maximum values of a given attribute. The formula can be expressed as:
\begin{equation}\label{eq:min-max}
    \hat{a} = low+ \frac{(high - low)*(a - minA)}{maxA - minA)} 
\end{equation}
where $\hat{a}$ represents the transformed value of an attribute A.

It has been pointed out that this method suffers from uncertainty when normalizing time series data\cite{dataNormalization2014}, however, given that in this context it is not used to normalize time series value it has been included in the analysis.
\paragraph{Decimal Scaling}\label{sec:data-modelling:scalers:max-abs}
The decimal scaling normalization method works by scaling each feature by its maximum absolute value and can be represented by:
\begin{equation}
    \hat{a} = \frac{a}{10^d}
\end{equation}
where $\hat{a}$ represents the transformed value, $a$, the original value and $d$ is the smallest integer where $max(|\hat{a}|<1)$.

Similarly to min-max normalization, as this method also relies on knowing the maximum value of an attribute ahead of time, it has been unsuitable to time series data\cite{dataNormalization2014}. However, again, the attributes scaled do not include time series data it has been deemed appropriate to include it in the course of the analysis.

\paragraph{Z-Score Normalization}\label{sec:data-modelling:scalers:standard}
This method standardizes the features by removing the mean and scaling the attribute to unit variance. It can be represented y:
\begin{equation}
    \hat{a} = \frac{a - \mu(a)}{\sigma(a)}
\end{equation}
where $\hat{a}$ represents the transformed value, $\mu(a)$ is the arithmetic mean of a given attribute and $\sigma(a)$ is its standard deviation.

As this method does not rely on knowing the minimum and maximum values ahead of applying the transformation it overcomes the deficiencies of the Min-Max and Decimal Scaling methods\cite{dataNormalization2014}. However, at the same time, it faces difficulties with a non-stationary data series where the mean or standard deviation change over time. Given that during the underlying analysis the data has been static and not continuously undergoing updates or other modifications it has been deemed an appropriate scaling method to include.

\paragraph{Power Transformation - Yeo-Johnson method} \label{sec:data-modelling:scalers:power-yeo-johnson}
Power data normalization methods are a family of parametric, monotonic transformations aiming to alter given data from any distribution to as close to a Gaussian distribution as possible. As with any data normalization method, it is performed on data in order to reduce variance and minimize skewness.

The Box-Cox method,depicted in equation \ref{eq:box-cox} has been first proposed in 1964\cite{boxCox1964} and has been a very popular method of dealing with distribution variance \cite{YANG200614}. However, it is only applicable to strictly positive data\cite{YANG200614}. 
Additionally, as pointed out by Yeo and Johnson \cite{yeo-johnson-original} despite the fact that Box-Cox method could be modified to approximate normal distribution for negative $\lambda$, however, in those instances the proposed modifications fail when applied to heavily skewed distributions.
\begin{equation}\label{eq:box-cox}
    \hat{a}^{(\lambda)}=\begin{cases} \frac {{a}^{\lambda } - 1 }{\lambda}& \text{if $\lambda \neq 0$},\\
    ln(a)& \text{if $\lambda=0$}.
\end{cases}
\end{equation}

Provided the data is indeed heavily skewed, but despite the fact that all of the values present in the dataset are positive, as per the domain, the Box-Cox method has been ruled out as an appropriate data normalization method. Instead, Yeo-Johnson transformation, as per equation\ref{eq:yeo-johnson}, has been applied.

\begin{equation}\label{eq:yeo-johnson}
    \hat{a}^{(\lambda)}=\begin{cases} {\{{a +1}^{\lambda } - 1 }\}/ {\lambda}& \text{if $a \geq 0$ and $\lambda \neq 0$},\\
    log(a+1)& \text{if $a\geq 0$ and $\lambda=0$},\\
    -\{(-a +1)^\lambda -1\}/\lambda & \text{if $a < 0$ and $\lambda \neq 0$},\\
    -log(-a +1) & \text{if $a < 0$ and $\lambda = 0$}.
    
\end{cases}
\end{equation}

\paragraph{Quantile Transformation Methods} \label{sec:data-modelling:scalers:quantile}
Quantile regression has been deemed appropriate for heavily skewed data\cite{quantileNormalizationInSkewedDataAnalysis}, therefore, it has been included as part of the analysis.

The method is intended to spread out the most frequent data points for a given attribute, thus reducing the impact of marginal outliers.

There are two methods applied as part of the underlying analysis, the uniform and the normal distribution variants. The uniform distribution variant maps the data to a distribution with values between 0 and 1, spread out, as method name suggests, uniformly.

The normal distribution variant utilizes the mean and median of a given attribute where the median of the input becomes the mean of the output, centered at 0 value. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Implementation}\label{sec:implementation}
The implementation has been carried out in two steps. In the first step, a tool has been developed in Python language for the purpose of gathering the data required for the analysis - the process is outlined in Section \ref{sec:impl-data-gatherer}. Secondly, the data has been analyzed, in accordance with steps defined in Section \ref{sec:design}, List \ref{lst:dataset-ops}, using Jupyter Notebook tooling, which is the topic of Section \ref{sec:impl-data-analysis}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{Chapters/03a-gatherer-implementation.tex}

\input{Chapters/03b-data-analysis-implementation.tex}
\input{Chapters/03c-feature-distribution.tex}
\input{Chapters/03d-outliers.tex}
\input{Chapters/03e-scalers-impl.tex}
\input{Chapters/03f-logitstic-regression-impl.tex}
\input{Chapters/03g-decision-tree-impl.tex}
