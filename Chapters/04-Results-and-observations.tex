\chapter{Results and Observations}\label{chp:results-and-observations}
\section{Logistic Regression Results}\label{sec:results:log-reg}
% \todo{Need to compare LogReg L1 and L2 results to those on non-regularized and see if there was any change}
% \todo{Need to Speak to why decision tree results were so much better than those of logistic regression}

\todo{- What are the results in, F1? TF/TP, RSS?}
Due to the implementation specification of the Logistic Regression algorithm forcing L2 regularization upon model by default it was not possible to obtain non-regularized results for comparison.

The results for the analysis of both upsampled and downsampled datasets, as well as L1 and L2 regularization methods have been compiled into Table \ref{tbl:results:log-reg}. From same it can be observed that the probability of a valid classification of \isBug{} target variable is between 52\% - 63.9\% between different scaling methods used to normalize given data.

On average the dataset where the upsampling method was used to achieve class balance has provided better results. 

From regularization point of view, L2 was more successful on average.

The probability of class 0 and 1 class, corresponding to a given file being a bug or not, has been consistently averaged at 50\% for each class across all datasets, normalization and regularization methods. 
\begin{table}[h!]
\centering
\caption{Logistic Regression Results}
\label{tbl:results:log-reg}
\begin{tabular}{@{}lllll@{}}
\toprule
 & Up L1 & Down L1 & Up L2 & Down L2 \\ \midrule
unmodified & 0.588 & 0.588 & 0.563 & 0.558 \\
min-max & 0.539 & 0.515 & 0.550 & 0.574 \\
max-abs & 0.528 & 0.544 & 0.509 & 0.504 \\
standard & 0.547 & 0.536 & 0.544 & 0.582 \\
power-yeo-johnson & 0.617 & 0.631 & 0.639 & 0.542 \\
quantile-normal & 0.536 & 0.563 & 0.604 & 0.617 \\
quantile-uniform & 0.609 & 0.571 & 0.625 & 0.625 \\ \bottomrule
\end{tabular}
\end{table}

\section{Decision Tree Results}\label{sec:results:decision-tree}
The results of decision tree classification have been compiled into Table \ref{tbl:results:decision-tree}. From same it can be observed, that similarly to Logistic Regression results in section \ref{sec:results:log-reg}, the upsampled dataset exhibits higher prediction scores. 

The results of the classification analysis would vary from approximately 86\% to just above 88\% for the downsampled dataset and reaching a staggering approximately 99\% accuracy in the upsampled dataset.
\begin{table}[h!]
\centering
\caption{Decision Tree Classification Results}
\label{tbl:results:decision-tree}
\begin{tabular}{@{}lll@{}}
\toprule
Method Name & Upsampled Score & Downsampled Score \\ \midrule
unmodified & 99.59 & 85.98 \\
min-max & 99.25 & 86.79 \\
max-abs & 99.37 & 88.14 \\
standard & 99.52 & 87.06 \\
power-yeo-johnson & 99.74 & 87.6 \\
quantile-normal & 99.66 & 87.87 \\
quantile-uniform & 99.37 & 87.6 \\ \bottomrule
\end{tabular}
\end{table}

\section{Comparison of model results}
From the results compiled for individual models, sections \ref{sec:results:log-reg}-\ref{sec:results:decision-tree}, it is clearly visible that the Decision Tree Classifier was significantly more successful at generating accurate predictions. It comes expected, as the decision tree model has been proven out to be very successful in analyzing datasets with high feature collinearity \cite{Bertsimas2017Cart} such as this one. 