\chapter{Results and Observations}\label{chp:results-and-observations}
It should be noted that due to the randomness of the resampling and scaling operations the exact same results will not be obtained between subsequent runs of the prediction model. However, the results obtained from multiple runs as part of the research have been of consistent magnitude.
\section{Logistic Regression Results}\label{sec:results:log-reg}

Due to the implementation specification of the Logistic Regression algorithm forcing L2 regularization upon the model by default it was not possible to obtain non-regularized results for comparison.

The scoring function provided by the underlying implementation of the Sci-Kit Learn library used defines it as the mean accuracy of correctly predicting the target value ($y$) for a given set of features ($X$). It is presented as a decimal number in the range of $ 0$ to $1$, however, for the purpose of defining it as percentage value all scores from underlying models have been multiplied by $100\%$.

The results for the analysis of both up-sampled and down-sampled datasets, as well as L1 and L2 regularization methods have been compiled into Table \ref{tbl:results:log-reg}. From same it can be observed that the probability of a valid classification of \isBug{} target variable is between 50.4\% - 63.9\% between different scaling methods used to normalize given data.

On average the dataset where the upsampling method was used to achieve class balance has provided better results. It is expected that the smaller number of features exhibiting skewed distribution, as per Section \ref{sec:impl-data-analysis:feature-dist-after-scaling}, had an impact on that. It should also be noted that the difference in accuracy between upsampled and downsampled datasets, for equivalent scalers, has been up to $\approx 3.8\%$.

From regularization point of view, L2 was more successful on average.

The probability of predicting 0 or 1 class, corresponding to a given file containing a bug or not, has been consistently averaged at 50\% for each class across all datasets, regardless or normalization and regularization methods utilized. 
\begin{table}[!h]
\centering
\caption{Logistic Regression Results}
\label{tbl:results:log-reg}
\begin{tabular}{lllll}
\hline
Scaler Method Name & \begin{tabular}[c]{@{}l@{}}L1\\ Upsampled\end{tabular} & \begin{tabular}[c]{@{}l@{}}L1 \\ Downsampled\end{tabular} & \begin{tabular}[c]{@{}l@{}}L2\\ Upsampled\end{tabular} & \begin{tabular}[c]{@{}l@{}}L2 \\ Downsampled\end{tabular} \\ \hline
unmodified & 58.8 & 58.8 & 56.3 & 55.8 \\
min-max & 53.9 & 51.5 & 55.0 & 57.4 \\
max-abs & 52.8 & 54.4 & 50.9 & 50.4 \\
standard & 54.7 & 53.6 & 54.4 & 58.2 \\
power-yeo-johnson & 61.7 & 63.1 & 63.9 & 63.6 \\
quantile-normal & 53.6 & 56.3 & 60.4 & 61.7 \\
quantile-uniform & 60.9 & 57.1 & 62.5 & 62.5 \\ \hline
\end{tabular}
\end{table}

\section{Decision Tree Results}\label{sec:results:decision-tree}
The results of decision tree classification have been compiled into Table \ref{tbl:results:decision-tree}. From same it can be observed, that similarly to Logistic Regression results in section \ref{sec:results:log-reg}, the upsampled dataset exhibits higher prediction scores. 

As has been the case with the Logistic Regression model, also in the case of Decision Tree Classifier the scoring function is defined as the mean accuracy of correctly predicting the target value ($y$) for a given set of features ($X$). It is again presented as a decimal number in the range of $ 0$ to $1$, however, for the purpose of defining it as percentage the scores have been multiplied by $100\%$.

The results of the classification analysis would vary from approximately 88\% to almost 91\% for the downsampled dataset and reaching a staggering 99\% accuracy in the upsampled dataset.
\begin{table}[h!]
\centering
\caption{Decision Tree Classification Results}
\label{tbl:results:decision-tree}
\begin{tabular}{@{}lll@{}}
\toprule
Scaler Method Name & Upsampled Score & Downsampled Score \\ \midrule
unmodified & 99.55& 89.22  \\
min-max & 99.07 & 90.57 \\
max-abs & 99.37 & 87.87 \\
standard & 99.18 & 88.68 \\
power-yeo-johnson & 99.37 & 88.68 \\
quantile-normal & 99.4 & 89.49 \\
quantile-uniform & 99.55 & 90.84 \\ \bottomrule
\end{tabular}
\end{table}

The difference in prediction accuracy is expected to have come from the size of the data sample under analysis. To reiterate after the Section \ref{sec:impl-data-analysis:resampling}, in the case of downsampled dataset there were only 1484 records available while the upsampled dataset had a far superior sample size of 10734. Given that the upsampled dataset is over seven times the size of the downsampled one it is expected that the former will provide greater prediction accuracy.

\section{Comparison of model results}
From the results compiled for individual models, sections \ref{sec:results:log-reg} and \ref{sec:results:decision-tree}, it is clearly visible that the Decision Tree Classifier was significantly more successful at obtaining accurate predictions. 

One of the factors bearing an effect on such result is the feature distribution in the given dataset, suffering from significant skewness. However, it has been consciously decided against redistributing the data before applying the Logistic Regression model and it is expected that this decision had an impact on the results of the linear model, which by its definition would perform better should all data be more normally distributed. It should be noted, as discussed in detail in Section \ref{sec:impl-data-analysis:feature-dist}, that in general the skewness of the data while present was not significant in most features.

Additional factor contributing to greater accuracy of a Decision Tree Classifier is that the model has been proven out to be very successful in analyzing datasets with high feature collinearity \cite{Bertsimas2017Cart} such as the one present in the underlying research use case. 