\subsubsection{Scaling Data}
The purpose of this section is to scale all of the data to the same range. 

First method utilized was using natural logarithm $log(e)$, which was conducted in steps described in detail as follows.
\paragraph{$log(e)$ - Determining which columns to scale}
First the dataset was copied in order to avoid polluting the original dataset. Then in order to ensure that all values are on approximately the same scale a list of columns where column variance was above third quartile of the overall variance values was compiled. This can be read from code snippet \ref{code:scalers:loge-scaler-getting-cols}, with the names of columns gathered are presented in Table \ref{tbl:available-data-non-repeating-types}.

\begin{code}
\captionof{listing}{Log(e) gathering columns for analysis}
\label{code:scalers:loge-scaler-getting-cols}
\begin{minted}[breaklines]{python}
variance_frame = pd.DataFrame(df.var())
variance_quartile_3 = collect_outlier_data_to_list(variance_frame)[0]['q3']
high_variance_cols=get_high_variance_cols(df, var_q3)
\end{minted}
\end{code}

\begin{table}[!h]
\centering
\caption{Columns with variance greater than Q3 of overall variance in the dataset}
\label{tab:loge-scaler-cols-aboveq3}
\begin{tabular}{@{}l@{}}
\toprule
Column name \\ \midrule
comment\_lines \\
comment\_lines\_density \\
complexity \\
it\_uncovered\_lines \\
lines\_to\_cover \\
ncloc \\
overall\_coverage \\
file\_age\_in\_sec \\ \bottomrule
\end{tabular}
\end{table}


\paragraph{$log(e)$ - Executing the scaler}
Subsequently \texttt{convert\_col\_vals\_to\_log} function was used to convert values in all columns collected to their natural logarithm representation, with code snippet \ref{code:scalers:loge-scaler-exec} depicting preparation for execution while \ref{code:scalers:loge-impl} describes the actual process of adding $_log$ suffixed columns to the copied dataset depicting the $log(e)$ value of a given original. 

\begin{code}
\captionof{listing}{Log(e) scaler implementation}
\label{code:scalers:loge-impl}
\begin{minted}[breaklines]{python}
def convert_col_vals_to_log(df, cols):
    for column in cols:
        df[column + '_log'] = np.log(df[column])
\end{minted}
\end{code}

\begin{code}
\captionof{listing}{Log(e) scaler execution}
\label{code:scalers:loge-scaler-exec}
\begin{minted}[breaklines]{python}
df_log = df.copy(deep=True)
convert_col_vals_to_log(df_log, high_variance_cols)
df_log.drop(high_variance_cols, axis=1, inplace=True)
df_log.var()
\end{minted}
\end{code}

\paragraph{$log(e)$ - Conclusion}
Table \ref{tbl:loge-scaler-cols-transformed} depicts the results of the transformation. While most of the obtained variance values are of a much smaller scale and slightly more in line with the remaining values, given that 75\% of values are below value of approximately 240, the transformed columns are now undervalued. 
Additionally the \texttt{nan} values indicate that a number could not be obtained as a result of the transformation, likely due to the small original variance value. 

\textbf{In conclusion} given that not all values are of the same scale at the end of this process, and given the presence of non-numerical values, \texttt{nan}, it has been decided against the usage of natural logarithm scaling method.
\begin{table}[!h]
\centering
\caption{Scaled variance results}
\label{tbl:loge-scaler-cols-transformed}
\begin{tabular}{@{}ll@{}}
\toprule
Column Name & Transformed Variance Value \\ \midrule
comment\_lines\_log & nan \\
comment\_lines\_density\_log & nan \\
complexity\_log & nan \\
it\_uncovered\_lines\_log & 0.23 \\
lines\_to\_cover\_log & 1.23 \\
ncloc\_log & 1.08 \\
overall\_coverage\_log & nan \\
file\_age\_in\_sec\_log & 5.49 \\ \bottomrule
\end{tabular}
\end{table}


\paragraph{Scaling using Sci-Kit Learn's built-in methods}
The following scaling methods will be applied sequentially to all data points subsequently used in the analysis:
\begin{itemize}
    \item Min Max, described in section \ref{sec:data-modelling:scalers:min-max}
    \item Max Abs - corresponds to Decimal Scaling method, section \ref{sec:data-modelling:scalers:max-abs}
    \item Standard, corresponds to Z-Score method, section \ref{sec:data-modelling:scalers:standard}
    \item Power Transformer using Yeo-Johnson method, section\ref{sec:data-modelling:scalers:power-yeo-johnson}
    \item Quantile Normal, section \ref{sec:data-modelling:scalers:quantile}
    \item Quantile Uniform, section \ref{sec:data-modelling:scalers:quantile}
\end{itemize}

Data frames containing scaled data will be compiled into a reference map for easy lookup of data.

Following scaled data collection, standard deviation for each column for each of the methods will be compared to check the data distribution. Decided on a numerical method as compiling graphs for all of 6 scaling methods times 41 features would be inefficient. Graphs may be compiled for illustrating the some of the distribution improvements.

Finally, each scaled dataset will be used in Logistic Regression and Decision Tree Algorithm for analysis.
\paragraph{Initial Setup}
\begin{code}
\captionof{listing}{Initial setup - constants}
\label{code:scalers:setup}
\begin{minted}[breaklines]{python}
# Constants
unmod_key='unmodified'
min_max_key='min-max'
max_abs_key = 'max-abs'
standard_key='standard'
yeo_johnson_key='power-yeo-johnson'
quantile_normal_key='quantile-normal'
quantile_uniform_key='quantile-uniform'

methods_order=[unmod_key, min_max_key, max_abs_key,standard_key, yeo_johnson_key, quantile_normal_key, quantile_uniform_key]

scaled_data_ref_map_downscaled={}
scaled_data_ref_map_upscaled={}
\end{minted}
\end{code}
\paragraph{Executing Scalers}
Executing and fitting all scalers to the dataset provided. Target column and well as text data columns have been dropped from scaling analysis.

Text columns dropped are pertaining to:
\begin{itemize}
    \item author
    \item previous author
    \item issue key
    \item file path
    \item source repository
\end{itemize}

All of the above have been retained in the dataset only for verification purposes to make sure that any transformed data point can be tracked back to its original state.

\begin{code}
\captionof{listing}{Initial setup - getting datasets and removal of text columns}
\label{code:scalers:setup-of-objects-and-drop-text-cols}
\begin{minted}[breaklines]{python}
df_upsampled, df_downsampled = get_up_and_downsample_df(df_transformed, 'is_bug', 123, [0,1])

X = df_downsampled.drop(['author', 'prev_author', 'issue_key', 'file_path', 'source_repo', 'is_bug'], axis=1)
y = df_downsampled['is_bug']

X_up = df_upsampled.drop(['author', 'prev_author', 'issue_key', 'file_path', 'source_repo', 'is_bug'], axis=1)
y_up = df_upsampled['is_bug']

# Initializing the unmodified data frame in reference map
scaled_data_ref_map_downscaled[unmod_key]=X.copy(deep=True)
scaled_data_ref_map_upscaled[unmod_key]=X_up.copy(deep=True)
\end{minted}
\end{code}

\begin{landscape}
\begin{code}
\captionof{listing}{Initial setup - getting datasets and removal of text columns}
\label{code:scalers:execute-scalers}
\begin{minted}[breaklines]{python}
# Min Max scaler
scaler = MinMaxScaler()
x_scaled = scaler.fit_transform(X)
x_upscaled = scaler.fit_transform(X_up)

scaled_data_ref_map_downscaled[min_max_key]=pd.DataFrame(x_scaled, columns=X.columns)
scaled_data_ref_map_upscaled[min_max_key]=pd.DataFrame(x_upscaled, columns=X_up.columns)

# maximal absolute scaler

# resetting X to default state
X = df_downsampled.drop(['author', 'prev_author', 'issue_key', 'file_path', 'source_repo', 'is_bug'], axis=1)
X_up = df_upsampled.drop(['author', 'prev_author', 'issue_key', 'file_path', 'source_repo', 'is_bug'], axis=1)

scaled_data_ref_map_downscaled[max_abs_key]=pd.DataFrame(MaxAbsScaler().fit_transform(X), columns=X.columns)
scaled_data_ref_map_upscaled[max_abs_key]=pd.DataFrame(MaxAbsScaler().fit_transform(X_up), columns=X_up.columns)

# Standard Scaler

# resetting X to default state
X = df_downsampled.drop(['author', 'prev_author', 'issue_key', 'file_path', 'source_repo', 'is_bug'], axis=1)
X_up = df_upsampled.drop(['author', 'prev_author', 'issue_key', 'file_path', 'source_repo', 'is_bug'], axis=1)

scaled_data_ref_map_downscaled[standard_key]=pd.DataFrame(StandardScaler(with_mean=False).fit_transform(X), columns=X.columns)
scaled_data_ref_map_upscaled[standard_key]=pd.DataFrame(StandardScaler(with_mean=False).fit_transform(X_up), columns=X_up.columns)

# Power Transformer - Yeo-Johnson method

# resetting X to default state
X = df_downsampled.drop(['author', 'prev_author', 'issue_key', 'file_path', 'source_repo', 'is_bug'], axis=1)
X_up = df_upsampled.drop(['author', 'prev_author', 'issue_key', 'file_path', 'source_repo', 'is_bug'], axis=1)

scaled_data_ref_map_downscaled[yeo_johnson_key]=pd.DataFrame(PowerTransformer(method='yeo-johnson').fit_transform(X), columns=X.columns)
scaled_data_ref_map_upscaled[yeo_johnson_key]=pd.DataFrame(PowerTransformer(method='yeo-johnson').fit_transform(X_up), columns=X_up.columns)


# Quantile Transformation - Normal

# resetting X to default state
X = df_downsampled.drop(['author', 'prev_author', 'issue_key', 'file_path', 'source_repo', 'is_bug'], axis=1)
X_up = df_upsampled.drop(['author', 'prev_author', 'issue_key', 'file_path', 'source_repo', 'is_bug'], axis=1)

scaled_data_ref_map_downscaled[quantile_normal_key]=pd.DataFrame(QuantileTransformer(output_distribution='normal').fit_transform(X), columns=X.columns)
scaled_data_ref_map_upscaled[quantile_normal_key]=pd.DataFrame(QuantileTransformer(output_distribution='normal').fit_transform(X_up), columns=X_up.columns)

# Quantile Transformation - Uniform

# resetting X to default state
X = df_downsampled.drop(['author', 'prev_author', 'issue_key', 'file_path', 'source_repo', 'is_bug'], axis=1)
X_up = df_upsampled.drop(['author', 'prev_author', 'issue_key', 'file_path', 'source_repo', 'is_bug'], axis=1)

scaled_data_ref_map_downscaled[quantile_uniform_key]=pd.DataFrame(QuantileTransformer(output_distribution='uniform').fit_transform(X), columns=X.columns)
scaled_data_ref_map_upscaled[quantile_uniform_key]=pd.DataFrame(QuantileTransformer(output_distribution='uniform').fit_transform(X_up), columns=X_up.columns)
\end{minted}
\end{code}
\end{landscape}

\paragraph{Comparing variance per column for each method}
Verifying that column variance for each of the scaler methods have been reduced on per column basis as well as mean and median standard deviation for each of the methods investigated. Overall it is reasonable to surmise that the variance have improved by a factor of approximately 200 (!!) per method implemented.

For some of the features the variance decrease can be even more dramatic, for example it uncovered lines feature originally had variance of a little over 1400, however, after scaling methods were applied it was reduced to single digit value.

\begin{code}
\captionof{listing}{Initial setup - getting datasets and removal of text columns}
\label{code:scalers:getting-metrics-downsample}
\begin{minted}[breaklines]{python}
print('Statistics for scaled dataset created by downsampling')
for method in scaled_data_ref_map_downscaled.keys():
    avg=np.average(scaled_data_ref_map_downscaled[method].describe().std())
    print('Average std for, {0},\t\t\t {1}'.format(method, avg))
    
for method in scaled_data_ref_map_downscaled.keys():
    med = np.median(scaled_data_ref_map_downscaled[method].describe().std())
    print('Median std for, {0},\t\t\t {1}'.format(method, med))
    \end{minted}
\end{code}

\begin{code}
\captionof{listing}{Initial setup - getting datasets and removal of text columns}
\label{code:scalers:getting-metrics-upsample}
\begin{minted}[breaklines]{python}

print('Statistics for scaled dataset created by upsampling')
for method in scaled_data_ref_map_upscaled.keys():
    avg=np.average(scaled_data_ref_map_upscaled[method].describe().std())
    print('Average std for, {0},\t\t\t {1}'.format(method, avg))
    
for method in scaled_data_ref_map_upscaled.keys():
    med = np.median(scaled_data_ref_map_upscaled[method].describe().std())
    print('Median std for, {0},\t\t\t {1}'.format(method, med))
        \end{minted}
\end{code}

Data series containing variance metrics for un-scaled, or unmodified data, has been removed as it was making it impossible to observe variance changes in scaled data due to its magnitude. Additionally when visualizing variance for unmodified data \fileAgeInSec{} feature has been removed as its extreme value was overshadowing all other variance metrics making them appear the same in magnitude.

Variance data for all scaling methods has been compiled into a single dataset for easy manipulation and visualization. Below it can be observed from the table that any scaling method decreased data variance metric significantly.