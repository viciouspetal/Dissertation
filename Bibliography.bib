@misc{fowlerOnMicroservices,
	author = "Fowler, Martin",
    title = "Microservices - a definition of this new architectural term",
    year = "2014",
    url = "https://martinfowler.com/articles/microservices.html"
}

@misc{costOfBugInRelationToDevelopmentPhase,
	author = {{Raygun Incorporated}},
    title = "How much could software errors be costing your company?",
    year = "2017",
    url = "https://raygun.com/blog/cost-of-software-errors"    
}
@article{dragoniOnMs,
	author = "Dragoni, Nicola and Giallorenzo, Saverio and Lafuente, Alberto Lluch and Mazzara, Manuel and Montesi, Fabrizio and Mustafin, Ruslan and Safina, Larisa",
    title = "Microservices: yesterday, today, and tomorrow",
    url = "https://arxiv.org/abs/1606.04036",
    year = "2016"
}

@INPROCEEDINGS{softwareSecurityMentionMicroservice, 
author={Sung Kim and F. B. Bastani and I-Ling Yen and Ing-Ray Chen}, 
booktitle={14th International Symposium on Software Reliability Engineering, 2003. ISSRE 2003.}, 
title={High-assurance synthesis of security services from basic microservices}, 
year={2003}, 
volume={}, 
number={}, 
pages={154-165}, 
keywords={Internet;certification;electronic mail;formal verification;object-oriented programming;security of data;Internet;component security certification;computer hackers;computer network security;computer systems;computer threats;e-mail application;information misusage;malicious attacks;malicious code embedding;microservices;monolithic software system;security service synthesis;software bugs;system security;Computer bugs;Computer hacking;Computer networks;Computer security;Data security;Embedded software;Explosives;Information security;Internet;Software systems}, 
doi={10.1109/ISSRE.2003.1251039}, 
ISSN={1071-9458}, 
month={Nov},}

@INPROCEEDINGS{understandingOfMicroservices, 
author={D. Shadija and M. Rezai and R. Hill}, 
booktitle={2017 23rd International Conference on Automation and Computing (ICAC)}, 
title={Towards an understanding of microservices}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-6}, 
keywords={Internet of Things;Web services;business data processing;service-oriented architecture;Internet of Things;SOA;business analysts;domain driven design;enterprise architects;microservices architecture;service oriented architecture;Business;Computer architecture;Data models;Internet of Things;Service-oriented architecture;Domain Driven Design (DDD);Service Oriented Architecture (SOA);Software Engineering;microservices}, 
doi={10.23919/IConAC.2017.8082018}, 
ISSN={}, 
month={Sept},

}@ARTICLE{studiesOnFaultPrediction, 
author={T. Hall and S. Beecham and D. Bowes and D. Gray and S. Counsell}, 
journal={IEEE Transactions on Software Engineering}, 
title={A Systematic Literature Review on Fault Prediction Performance in Software Engineering}, 
year={2012}, 
volume={38}, 
number={6}, 
pages={1276-1304}, 
abstract={Background: The accurate prediction of where faults are likely to occur in code can help direct test effort, reduce costs, and improve the quality of software. Objective: We investigate how the context of models, the independent variables used, and the modeling techniques applied influence the performance of fault prediction models. Method: We used a systematic literature review to identify 208 fault prediction studies published from January 2000 to December 2010. We synthesize the quantitative and qualitative results of 36 studies which report sufficient contextual and methodological information according to the criteria we develop and apply. Results: The models that perform well tend to be based on simple modeling techniques such as Naive Bayes or Logistic Regression. Combinations of independent variables have been used by models that perform well. Feature selection has been applied to these combinations when models are performing particularly well. Conclusion: The methodology used to build models seems to be influential to predictive performance. Although there are a set of fault prediction studies in which confidence is possible, more studies are needed that use a reliable methodology and which report their context, methodology, and performance comprehensively.}, 
keywords={Bayes methods;regression analysis;software fault tolerance;software quality;contextual information;cost reduction;fault prediction models;fault prediction performance;fault prediction study;feature selection;independent variables;logistic regression;methodological information;naive Bayes;predictive performance;reliable methodology;simple modeling techniques;software engineering;software quality;systematic literature review;Analytical models;Context modeling;Data models;Fault diagnosis;Predictive models;Software testing;Systematics;Systematic literature review;software fault prediction}, 
doi={10.1109/TSE.2011.103}, 
ISSN={0098-5589}, 
month={Nov},}

@INPROCEEDINGS{autoDetectionOfPerfBugs, 
author={S. Tsakiltsidis and A. Miranskyy and E. Mazzawi}, 
booktitle={2016 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)}, 
title={On Automatic Detection of Performance Bugs}, 
year={2016}, 
volume={}, 
number={}, 
pages={132-139}, 
abstract={Context: Software performance is a critical non-functional requirement, appearing in many fields such as mission critical applications, financial, and real time systems. In this work we focused on early detection of performance bugs, our software under study was a real time system used in the mobile advertisement / marketing domain. Goal: Find a simple and easy to implement solution, predicting performance bugs. Method: We built several models using four machine learning methods, commonly used for defect prediction: C4.5 Decision Trees, Naive Bayes, Bayesian Networks, and Logistic Regression. Results: Our empirical results show that a C4.5 model, using lines of code changed, file's age and size as explanatory variables, can be used to predict performance bugs (recall = 0.73, accuracy = 0.85, and precision = 0.96). We show that reducing the number of changes delivered on a commit, can decrease the chance of performance bug injection. Conclusions: We believe that our approach can help practitioners to eliminate performance bugs early in the development cycle. Our results are also of interest to theoreticians, establishing a link between functional bugs and (non-functional) performance bugs, and explicitly showing that attributes used for prediction of functional bugs can be used for prediction of performance bugs.}, 
keywords={Bayes methods;belief networks;decision trees;learning (artificial intelligence);program debugging;real-time systems;regression analysis;software performance evaluation;Bayesian networks;C4.5 decision trees;Naive Bayes method;automatic performance bug detection;defect prediction;development cycle;explanatory variables;functional bugs;logistic regression;machine learning methods;marketing domain;mobile advertisement;performance bug elimination;performance bug injection;real time system;software performance;Computer bugs;Computer languages;Mobile communication;Predictive models;Real-time systems;Software;Testing;Mobile Advertisement;Performance Bugs;Performance Optimization;Quality Assurance;Software Faults}, 
doi={10.1109/ISSREW.2016.43}, 
ISSN={}, 
month={Oct},}

@INPROCEEDINGS{bugDetectionInParticleSwarm, 
author={A. Reungsinkonkarn and P. Apirukvorapinit}, 
booktitle={2015 6th International Conference on Intelligent Systems, Modelling and Simulation}, 
title={Bug Detection Using Particle Swarm Optimization with Search Space Reduction}, 
year={2015}, 
volume={}, 
number={}, 
pages={53-57}, 
abstract={A bug detection tool is an important tool in software engineering development. Many research papers have proposed techniques for detecting software bug, but there are certain semantic bugs that are not easy to detect. In our views, a bug can occur from incorrect logics that when a program is executed with a particular input, the program will behave in unexpected ways. In this paper, we propose a method and tool for software bugs detection by finding such input that causes an unexpected output guided by the fitness function. The method uses a Hierarchical Similarity Measurement Model (HSM) to help create the fitness function to examine a program behavior. Its tool uses Particle Swarm Optimization (PSO) with Search Space Reduction (SSR) to manipulate input by contracting and eliminating unfavorable areas of input search space. The programs under experiment were selected from four different domains such as financial, decision support system, algorithms and machine learning. The experimental result shows a significant percentage of success rate up to 93% in bug detection, compared to an estimated success rate of 28% without SSR.}, 
keywords={particle swarm optimisation;program debugging;search problems;software engineering;HSM;PSO;SSR;bug detection tool;fitness function;hierarchical similarity measurement model;input search space;particle swarm optimization;program behavior;search space reduction;software bug detection;software engineering development;Atmospheric measurements;Complexity theory;Computational modeling;Computer bugs;Particle measurements;Particle swarm optimization;Software;Bug Detection;Fitness Function;Hierarchical Similarity Measurement Model (HSM);Optimization;Particle Swarm Optimization (PSO)}, 
doi={10.1109/ISMS.2015.20}, 
ISSN={2166-0662}, 
month={Feb},}

@INPROCEEDINGS{duplicateBugDetection, 
author={J. Deshmukh and A. K. M and S. Podder and S. Sengupta and N. Dubash}, 
booktitle={2017 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
title={Towards Accurate Duplicate Bug Retrieval Using Deep Learning Techniques}, 
year={2017}, 
volume={}, 
number={}, 
pages={115-124}, 
abstract={Duplicate Bug Detection is the problem of identifying whether a newly reported bug is a duplicate of an existing bug in the system and retrieving the original or similar bugs from the past. This is required to avoid costly rediscovery and redundant work. In typical software projects, the number of duplicate bugs reported may run into the order of thousands, making it expensive in terms of cost and time for manual intervention. This makes the problem of duplicate or similar bug detection an important one in Software Engineering domain. However, an automated solution for the same is not quite accurate yet in practice, in spite of many reported approaches using various machine learning techniques. In this work, we propose a retrieval and classification model using Siamese Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM) for accurate detection and retrieval of duplicate and similar bugs. We report an accuracy close to 90% and recall rate close to 80%, which makes possible the practical use of such a system. We describe our model in detail along with related discussions from the Deep Learning domain. By presenting the detailed experimental results, we illustrate the effectiveness of the model in practical systems, including for repositories for which supervised training data is not available.}, 
keywords={feedforward neural nets;information retrieval;learning (artificial intelligence);pattern classification;program debugging;software development management;CNN;Deep Learning domain;Deep Learning techniques;Duplicate Bug Detection;LSTM;Long Short Term Memory;Siamese Convolutional Neural Networks;Software Engineering domain;accurate detection;classification model;costly rediscovery;duplicate Bug retrieval;existing bug;machine learning techniques;newly reported bug;original bugs;redundant work;similar bug detection;supervised training data;typical software projects;Computational modeling;Computer bugs;Machine learning;Neural networks;Sun;Training;Convolutional Neural Networks;Deep Learning;Duplicate Bug Detection;Information Retrieval;Long Short Term Memory;Natural Language Processing;Siamese Networks;Word Embeddings}, 
doi={10.1109/ICSME.2017.69}, 
ISSN={}, 
month={Sept},}

@INPROCEEDINGS{reopenedBugsAndMaintenence, 
author={E. Shihab and A. Ihara and Y. Kamei and W. M. Ibrahim and M. Ohira and B. Adams and A. E. Hassan and K. i. Matsumoto}, 
booktitle={2010 17th Working Conference on Reverse Engineering}, 
title={Predicting Re-opened Bugs: A Case Study on the Eclipse Project}, 
year={2010}, 
volume={}, 
number={}, 
pages={249-258}, 
abstract={Bug fixing accounts for a large amount of the software maintenance resources. Generally, bugs are reported, fixed, verified and closed. However, in some cases bugs have to be re-opened. Re-opened bugs increase maintenance costs, degrade the overall user-perceived quality of the software and lead to unnecessary rework by busy practitioners. In this paper, we study and predict re-opened bugs through a case study on the Eclipse project. We structure our study along 4 dimensions: (1) the work habits dimension (e.g., the weekday on which the bug was initially closed on), (2) the bug report dimension (e.g., the component in which the bug was found) (3) the bug fix dimension (e.g., the amount of time it took to perform the initial fix) and (4) the team dimension (e.g., the experience of the bug fixer). Our case study on the Eclipse Platform 3.0 project shows that the comment and description text, the time it took to fix the bug, and the component the bug was found in are the most important factors in determining whether a bug will be re-opened. Based on these dimensions we create decision trees that predict whether a bug will be re-opened after its closure. Using a combination of our dimensions, we can build explainable prediction models that can achieve 62.9% precision and 84.5% recall when predicting whether a bug will be re-opened.}, 
keywords={decision trees;program debugging;software maintenance;decision trees;eclipse project;software maintenance resource;team dimension;text description;Accuracy;Computer bugs;Data mining;Databases;Decision trees;Predictive models;Software;Re-opened bugs;software quality}, 
doi={10.1109/WCRE.2010.36}, 
ISSN={1095-1350}, 
month={Oct},
url={https://ieeexplore.ieee.org/document/5645566}}

@ARTICLE{softwareTestingChallenges, 
author={D. R. Kuhn and D. R. Wallace and A. M. Gallo}, 
journal={IEEE Transactions on Software Engineering}, 
title={Software fault interactions and implications for software testing}, 
year={2004}, 
volume={30}, 
number={6}, 
pages={418-421}, 
abstract={Exhaustive testing of computer software is intractable, but empirical studies of software failures suggest that testing can in some cases be effectively exhaustive. We show that software failures in a variety of domains were caused by combinations of relatively few conditions. These results have important implications for testing. If all faults in a system can be triggered by a combination of n or fewer parameters, then testing all n-tuples of parameters is effectively equivalent to exhaustive testing, if software behavior is not dependent on complex event sequences and variables have a small set of discrete values.}, 
keywords={failure analysis;program testing;software fault tolerance;statistical analysis;computer software testing;discrete value;event sequence;software behavior;software failure;statistical method;test design;Databases;Drugs;Embedded system;Fault detection;History;Microwave ovens;Software quality;Software testing;System testing;65;Statistical methods;test design.;testing strategies}, 
doi={10.1109/TSE.2004.24}, 
ISSN={0098-5589}, 
month={June},}

@inproceedings{Zhou_2012_whereShouldBugsBeFixed,
 author = {Zhou, Jian and Zhang, Hongyu and Lo, David},
 title = {Where Should the Bugs Be Fixed? - More Accurate Information Retrieval-based Bug Localization Based on Bug Reports},
 booktitle = {Proceedings of the 34th International Conference on Software Engineering},
 series = {ICSE '12},
 year = {2012},
 isbn = {978-1-4673-1067-3},
 location = {Zurich, Switzerland},
 pages = {14--24},
 numpages = {11},
 url = {http://dl.acm.org/citation.cfm?id=2337223.2337226},
 acmid = {2337226},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
} 

@INPROCEEDINGS{predicting-severity-of-a-bug, 
author={A. {Lamkanfi} and S. {Demeyer} and E. {Giger} and B. {Goethals}}, 
booktitle={2010 7th IEEE Working Conference on Mining Software Repositories (MSR 2010)}, 
title={Predicting the severity of a reported bug}, 
year={2010}, 
volume={}, 
number={}, 
pages={1-10}, 
abstract={The severity of a reported bug is a critical factor in deciding how soon it needs to be fixed. Unfortunately, while clear guidelines exist on how to assign the severity of a bug, it remains an inherent manual process left to the person reporting the bug. In this paper we investigate whether we can accurately predict the severity of a reported bug by analyzing its textual description using text mining algorithms. Based on three cases drawn from the open-source community (Mozilla, Eclipse and GNOME), we conclude that given a training set of sufficient size (approximately 500 reports per severity), it is possible to predict the severity with a reasonable accuracy (both precision and recall vary between 0.65-0.75 with Mozilla and Eclipse; 0.70-0.85 in the case of GNOME).}, 
keywords={data mining;program debugging;public domain software;reported bug;severity prediction;textual description;text mining algorithms;open source community;Mozilla;Eclipse;GNOME;Text mining;Computer bugs;Computer crashes;Guidelines;Algorithm design and analysis;Programming;Software systems;Seals;Software debugging;Computer architecture}, 
doi={10.1109/MSR.2010.5463284}, 
ISSN={2160-1852}, 
month={May},}

@article{Hastie2009SpringerOfLasso,
    title = {{Springer Series in Statistics The Elements of}},
    year = {2009},
    journal = {The Mathematical Intelligencer},
    author = {Hastie, Trevor and Tibsharani, Robert and Friedman, Jerome},
    number = {2},
    pages = {68-69},
    volume = {27},
    url = {http://www.springerlink.com/index/D7X7KX6772HQ2135.pdf},
    isbn = {9780387848570},
    doi = {10.1007/b94608},
    issn = {03436993},
    pmid = {15512507},
    arxivId = {arXiv:1011.1669v3}
}

@article{Hastie2009SpringerOfRidge,
    title = {{Springer Series in Statistics The Elements of}},
    year = {2009},
    journal = {The Mathematical Intelligencer},
    author = {Hastie, Trevor and Tibsharani, Robert and Friedman, Jerome},
    number = {2},
    pages = {61-68},
    volume = {27},
    url = {http://www.springerlink.com/index/D7X7KX6772HQ2135.pdf},
    isbn = {9780387848570},
    doi = {10.1007/b94608},
    issn = {03436993},
    pmid = {15512507},
    arxivId = {arXiv:1011.1669v3}
}

@article{Hastie2009OnLogisticRegForBinaryTargetClass,
    title = {{Springer Series in Statistics The Elements of}},
    year = {2009},
    journal = {The Mathematical Intelligencer},
    author = {Hastie, Trevor and Tibsharani, Robert and Friedman, Jerome},
    number = {2},
    pages = {119},
    volume = {27},
    url = {http://www.springerlink.com/index/D7X7KX6772HQ2135.pdf},
    isbn = {9780387848570},
    doi = {10.1007/b94608},
    issn = {03436993},
    pmid = {15512507},
    arxivId = {arXiv:1011.1669v3}
}

@INPROCEEDINGS{dataNormalization2014,
author={Nayak, S C and Misra, B B and Behera, H S},
booktitle={International Journal of Computer Information Systems and Industrial Management Applications},
title={Impact of Data Normalization on Stock Index Forecasting},
year={2014},
volume={6},
number={},
pages={257-269},
abstract={},
doi={}, 
ISSN={2150-7988}, 
month={},
}

@INPROCEEDINGS{quantileNormalizationInSkewedDataAnalysis,
author={Grace Hong, Hyokyoung},
booktitle={Computational Statistics and Data Analysis},
title={A quantile approach to the power transformed location-scale model},
year={2013},
volume={63},
number={},
pages={50-62},
abstract={},
doi={10.1016/j.csda.2013.01.022}, 
ISSN={01679473}, 
month={February},
url={https://www.sciencedirect.com/science/article/pii/S0167947313000376#br000040}
}

@article{yeo-johnson-original,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2673623},
 abstract = {We introduce a new power transformation family which is well defined on the whole real line and which is appropriate for reducing skewness and to approximate normality. It has properties similar to those of the Box-Cox transformation for positive variables. The large-sample properties of the transformation are investigated in the contect of a single random sample},
 author = {In-Kwon Yeo and Richard A. Johnson},
 journal = {Biometrika},
 number = {4},
 pages = {954--959},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {A New Family of Power Transformations to Improve Normality or Symmetry},
 volume = {87},
 year = {2000},
 url={https://www-jstor-org.ucc.idm.oclc.org/stable/2673623}
}

@article{YANG200614,
title = "A modified family of power transformations",
journal = "Economics Letters",
volume = "92",
number = "1",
pages = "14 - 19",
year = "2006",
issn = "0165-1765",
doi = "https://doi.org/10.1016/j.econlet.2006.01.011",
url = "http://www.sciencedirect.com/science/article/pii/S0165176506000188",
author = "Zhenlin Yang",
keywords = "Box–Cox transformation, Dual power transformation, Duration, Trans-normal",
abstract = "A modified family of power transformation, called the dual power transformation, is proposed. The new transformation is shown to possess properties similar to those of the well-known Box–Cox power transformation, but overcomes the long-standing truncation problem of the latter. It generates a rich family of distributions that is seen to be very useful in modeling and analysis of durations and event-times."
}
@article{boxCox1964,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2984418},
 abstract = {In the analysis of data it is often assumed that observations y<sub>1</sub>, y<sub>2</sub>, ..., y<sub>n</sub> are independently normally distributed with constant variance and with expectations specified by a model linear in a set of parameters θ. In this paper we make the less restrictive assumption that such a normal, homoscedastic, linear model is appropriate after some suitable transformation has been applied to the y's. Inferences about the transformation and about the parameters of the linear model are made by computing the likelihood function and the relevant posterior distribution. The contributions of normality, homoscedasticity and additivity to the transformation are separated. The relation of the present methods to earlier procedures for finding transformations is discussed. The methods are illustrated with examples.},
 author = {G. E. P. Box and D. R. Cox},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {2},
 pages = {211--252},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {An Analysis of Transformations},
 volume = {26},
 year = {1964}
}



@INPROCEEDINGS{auto-bug-fixing, 
author={A. {Arcuri} and }, 
booktitle={2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational Intelligence)}, 
title={A novel co-evolutionary approach to automatic software bug fixing}, 
year={2008}, 
volume={}, 
number={}, 
pages={162-168}, 
abstract={Many tasks in software engineering are very expensive, and that has led the investigation to how to automate them. In particular, software testing can take up to half of the resources of the development of new software. Although there has been a lot of work on automating the testing phase, fixing a bug after its presence has been discovered is still a duty of the programmers. In this paper we propose an evolutionary approach to automate the task of fixing bugs. This novel evolutionary approach is based on co-evolution, in which programs and test cases co-evolve, influencing each other with the aim of fixing the bugs of the programs. This competitive co-evolution is similar to what happens in nature for predators and prey. The user needs only to provide a buggy program and a formal specification of it. No other information is required. Hence, the approach may work for any implementable software. We show some preliminary experiments in which bugs in an implementation of a sorting algorithm are automatically fixed.}, 
keywords={formal specification;program debugging;program testing;sorting;coevolutionary approach;automatic software bug fixing;software engineering;software testing;buggy program;formal specification;sorting algorithm;Computer bugs;Software;Software testing;Formal specifications;Distance measurement;Training;Sorting}, 
doi={10.1109/CEC.2008.4630793}, 
ISSN={1089-778X}, 
month={June},}

@InProceedings{ensembleMethodsInMachineLearningDietterich,
author="Dietterich, Thomas G.",
title="Ensemble Methods in Machine Learning",
booktitle="Multiple Classifier Systems",
year="2000",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="1--15",
doi={https://doi.org/10.1007/3-540-45014-9_1},
url={https://link.springer.com/chapter/10.1007/3-540-45014-9_1},
abstract="Ensemble methods are learning algorithms that construct a set of classifiers and then classify new data points by taking a (weighted) vote of their predictions. The original ensemble method is Bayesian averaging, but more recent algorithms include error-correcting output coding, Bagging, and boosting. This paper reviews these methods and explains why ensembles can often perform better than any single classifier. Some previous studies comparing ensemble methods are reviewed, and some new experiments are presented to uncover the reasons that Adaboost does not overfit rapidly.",
isbn="978-3-540-45014-6"
}

@INPROCEEDINGS{breimanCart1984,
author={Breiman, Leo and Freidman, Jerome H and Olshen, Richard A and Stone, Charles J},
booktitle={Classification and Regression rees},
year={1984},
volume={},
number={},
pages={},
abstract={},
doi={https://doi.org/10.1201/9781315139470}, 
ISBN={978-0-412-04841-8}, 
month={},
publisher="Chapman \& Hall",
}

@article{ridgeEstInLogReg,
 ISSN = {00359254, 14679876},
 URL = {http://www.jstor.org/stable/2347628},
 abstract = {In this paper it is shown how ridge estimators can be used in logistic regression to improve the parameter estimates and to diminish the error made by further predictions. Different ways to choose the unknown ridge parameter are discussed. The main attention focuses on ridge parameters obtained by cross-validation. Three different ways to define the prediction error are considered: classification error, squared error and minus log-likelihood. The use of ridge regression is illustrated by developing a prognostic index for the two-year survival probability of patients with ovarian cancer as a function of their deoxyribonucleic acid (DNA) histogram. In this example, the number of covariates is large compared with the number of observations and modelling without restrictions on the parameters leads to overfitting. Defining a restriction on the parameters, such that neighbouring intervals in the DNA histogram differ only slightly in their influence on the survival, yields ridge-type parameter estimates with reasonable values which can be clinically interpreted. Furthermore the model can predict new observations more accurately.},
 author = {S. Le Cessie and J. C. Van Houwelingen},
 journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
 number = {1},
 pages = {191--201},
 publisher = {[Wiley, Royal Statistical Society]},
 title = {Ridge Estimators in Logistic Regression},
 volume = {41},
 year = {1992}
}

